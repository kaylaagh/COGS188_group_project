{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINESWEEPER\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Keyi Yu\n",
    "- Fatima Dong\n",
    "- Kayla Huynh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This project aims to build an AI model that solves the game Minesweeper using reinforcement learning algorithms. Minesweeper is a logic based game where the goal is to uncover all non-mine cells while avoiding mines. The game environment is a grid where each cell can either contain a mine or be safe. The numbers on the revealed cells tell you the number of adjacent mines. If you accidentally uncover the mine cell, then the game will end. The dataset is generated from a Python-based Minesweeper implementation, consisting of game states, player actions, and board configurations.The mindsweeper solver will operate as a rational agent by optimizing its performance uncovering safe cells, in the stochastic environment. We will be using search algorithms and heuristic strategies to effectively solve the problem. To evaluate performance, we will use win rate and average game duration as key metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Minesweeper is a single-player game where players must uncover all safe tiles without clicking on a mine! Safe squares provide numerical clues indicating how many mines are within a one-tile radius. The game features three official board sizes: Beginner (8×8 grid with 10 mines), Intermediate (16×16 grid with 40 mines), and Expert (16×30 grid with 90 mines). Clicking on a mine ends the game immediately, while selecting a safe tile may reveal numbers or automatically clear nearby squares. The challenge relies on using logic while simultaneously minimizing the risk of guessing. To win, all non-mine squares must be uncovered, leaving only the mines flagged.\n",
    "<br> <br>\n",
    "According to Kaye, “the Minesweeper problem is NP-Complete”, meaning that it is highly unlikely that there is an efficient algorithm that can solve it and that it is just as difficult as any other NP-Complete problem (like the traveling salesman problem)[1]. In other words, there is no known algorithm that can solve Minesweeper in polynomial time. Despite that some parts of the board can be determined through logical reasoning, some configurations require probabilistic guessing, making Minesweeper a constraint satisfaction problem[2].\n",
    "<br> <br>\n",
    "Given these papers, reinforcement learning will be explored in this project. There exists research, where Monte Carlo Simulation was used to solve the Minesweeper problem[3]. Another study showed that a mix of optimal heuristics proved to solve the problem with more efficiency. Such heuristics included: targeting the corners of the grid based on a previous study[2] due to the fact that the density of a mine in a corner is lower than any other tiles, maximizing the probability of revealing at least one safe-block in one move, and maximizing the expected number of safe blocks in the next move[4]. This greedy heuristic algorithm was named PSEQ. In a different study, double deep-Q-network was applied to the problem[5]. For this project, we will focus on implementing the Q-learning algorithm and additional heuristic(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Mindsweeper is a puzzle game where it requires players to uncover safe cells while avoiding the mine cells using the adjacent numerical clues. The goal of the game is to uncover the whole entire board without touching a mine. This game requires different strategies and sometimes risk taking when you run out of clues. You have to be able to make the most optimal decision within the stochastic environment where you don’t know where the mines are. The problem we are addressing is developing an AI-based Minesweeper solver that will be the most efficient when in play, using search algorithms and heuristic strategies. This problem is quantifiable since we are able to perform probability calculations and logical inference. It is also measurable since we can calculate the performance based on win rates and average game durations. Lastly, it is replicable since each time you play it is a different game board, allowing us to consistently use our algorithm since the rules do not change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data for this project will be generated using a Pygame-based Minesweeper implementation (https://pypi.org/project/pygame-minesweeper/ ). The dataset consists of game states, score ranking, and the final outcome, whether a win or loss. This implementation has various board configurations, such as Basic (10x10 grid with 10 mines), Intermediate (16x16 grid with 40 mines), Expert (6x30 grid with 99 mines), and Custom (users can define the number of rows, columns, and mines). Each board state is represented as a 2D grid where cells can be hidden. They can be revealed with a number indicating the count of adjacent mines, or flagged as a potential mine by the solver. The data collection process involves the solver interacting with the Minesweeper game by tracking wins or loses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "TD-Learning:\n",
    "\n",
    "\n",
    "DQN:\n",
    "\n",
    "We will implement the Deep Q-Networks (DQN), a reinforcement learning algorithm that uses a neural network to approximate Q-values for state-action pairs for solving the Mineweeper game. DQN was chosen because it can handle large state spaces and delayed reward to allow the agent to learn the optimal strategy over time. We incorporated experience replay,  a target network, and exploration-exploitation trade-off to stabilize training and also prevent catastrophic forgetting. Additionally, we designed a reward function that encourages the agent to prioritize safer moves (e.g. opening tiles near revealed low-numbered tiles and prioritizing corners). We believe that by adjusting the hyperparameters and optimizing the exploration and reward strategies, DQN was able to learn and significantly improve the performance which ultimately achieved a high win rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "For the evaluation of the performance of our Minesweeper solver, we plan to utilize the win rate and average game duration as our evaluation metrics. For the win rate, it is calculated as the percentage of games won out of the total games played. Mathematically, this is represented as:\n",
    "\n",
    "Win Rate = (Number of Games Won/ Total Games Played) * 100\n",
    "\n",
    "A higher win rate indicates better performance in solving Minesweeper boards. For the average game duration, we plan to measure how long it takes for the solver to complete a game which is already implemented in the Pygame Minesweeper. Shorter game duration for a successful game indicates better performance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Random Agent Implementation:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "Based on our plots, it is evident that DQN significantly outperformed both TD Learning and Random agents in completing the Minesweeper game. One key reason for this is DQN’s use of experience replay. It stores past experiences as tuples of (state, action, reward, next state) in a dataset called replay memory and randomly samples from this memory during training. As a result, DQN demonstrated improved performance with each consecutive episode, as shown in the Win Rate Over Episodes plot.\n",
    "\n",
    "Additionally, experience replay enabled DQN to maintain a balance between exploration and exploitation. The Exploration Rate plot illustrates how the exploration rate gradually decreased over time, allowing the agent to shift toward exploitation strategies that optimized its chances of winning. This trend is consistent across all plots, demonstrating how DQN’s epsilon-greedy strategy facilitated steady learning and ultimately led to consistent success in Minesweeper.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "One limitation to the random agent is the fact that it does not learn from previous games which isn’t good since there is no room for improvement. The random agents goes into the game blindly with no strategy, making moves based off of luck. Another limitation is the variance in the game. Since the random agent depends on luck, one game can be very short by clicking on the mine within one or two moves. On the other hand there could be a more successful game where the random agent gets really lucky with its moves and avoids the mine for longer.   \n",
    "\n",
    "A key limitation of the TD Learning agent in Minesweeper is that the board environment is partially observable. Since the agent does not have full visibility of the game state, it struggles to accurately evaluate its position and make optimal decisions. TD Learning updates its value estimates based on immediate rewards, which is not ideal in Minesweeper, as success requires a multi-step strategy rather than short-term rewards. Additionally, Minesweeper presents a unique challenge: stepping on a mine results in an automatic loss. This makes it difficult for TD Learning to effectively balance exploration and exploitation.\n",
    "\n",
    "One of the main limitations of the current DQN implementation is the instability in the learning curve, where there are sudden drops in rewards even after periods of learning. This suggests that the agent occasionally fails to generalize its learned strategy and potentially gets trapped in suboptimal policies or overfitting to specific patterns. Additionally, since Minesweeper has inherent uncertainty (e.g., forced guessing when no safe move is available), the agent may struggle in situations where logical deduction is limited, and lead to inconsistent performance across episodes.\n",
    "\n",
    "\n",
    "### Future work\n",
    "To improve the stability of learning and address the sudden drops in rewards, future work could explore techniques like prioritized experience replay, which ensures the agent learns more effectively from critical experiences rather than sampling past experiences uniformly. Another promising direction is integrating probabilistic reasoning into the model, where the agent can compute the likelihood of a tile being a mine based on nearby revealed numbers. This could be achieved by either modifying the state representation to include probability estimates or by incorporating a hybrid approach that combines reinforcement learning with rule-based decision-making. Ultimately, the goal is to develop a more stable and intelligent Minesweeper solver that can handle uncertainty more effectively while maintaining consistent performance improvements over time.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "Minesweeper itself is a pretty ethical game. We will not be using any data that would intrude personal privacy. Our solver will not conflict with any other player’s experience since this is only a single player game. The only concerns that we could possibly have is with our AI methods, where our solution would defeat the purpose of the game and be too overpowered. To prevent any issues, we will make sure to state all the methods that we use and be transparent as possible.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Our project demonstrates the significant benefits of utilizing Deep Q-Network (DQN) for training the optimal solution for the Mineaweeper compared to the temporal difference learning (TD learning). Unlike the other two models, DQN uses its neural network and experience replay to generalize across board states and finally find the optimal solution. This project shows the power of deep reinforcement learning in solving complex decision-making problems like Minesweeper. However, our model still needs further improvement to stabilize training and enhance performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "1.^: Kaye, R. (2000). \"Minesweeper is NP-Complete.\" Retrieved February 13, 2025. From https://academic.timwylie.com/17CSCI4341/minesweeper_kay.pdf\n",
    "2.^:Studholme, C. (2000). Minesweeper as a Constraint Satisfaction Problem. Unpublished project report. Retrieved February 13, 2025. From https://www.cs.toronto.edu/~cvs/minesweeper/minesweeper.pdf\n",
    "3.^:Qing, Y. et al. (2020). Critical exponents and the universality class of a minesweeper percolation model. International Journal of Modern Physics C, Volume 31, Issue 9, id. 2050129. Retrieved February 13, 2025. From https://ui.adsabs.harvard.edu/abs/2020IJMPC..3150129Q/abstract DOI: 10.1142/S0129183120501296\n",
    "4.^:Tu, J. (n.d.). Exploring Efficient Strategies for Minesweeper. Retrieved February 13, 2025. From https://cdn.aaai.org/ocs/ws/ws0294/15091-68459-1-PB.pdf .\n",
    "5.^: Smulders, B.G.J.P.( 25 Jun 2023) Optimizing minesweeper and its hexagonal variant with deep reinforcement learning. Retrieved February 13, 2025. From https://pure.tue.nl/ws/portalfiles/portal/307404348/Thesis_BDS_Juli_G._Smulders.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
